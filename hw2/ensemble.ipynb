{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2 Ensemble Learning\n",
    "#### CS 6350: HW 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[36 points] We will implement the boosting and bagging algorithms based on decision\n",
    "trees. Let us test them on the bank marketing dataset in HW1 (bank.zip in Canvas).\n",
    "We use the same approach to convert the numerical features into binary ones. That\n",
    "is, we choose the media (NOT the average) of the attribute values (in the training set)\n",
    "as the threshold, and examine if the feature is bigger (or less) than the threshold. For\n",
    "simplicity, we treat “unknown” as a particular attribute value, and hence we do not\n",
    "have any missing attributes for both training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### A. decision stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8 points] Modify your decision tree learning algorithm to learn decision stumps\n",
    "— trees with only two levels. Specifically, compute the information gain to select\n",
    "the best feature to split the data. Then for each subset, create a leaf node. Note\n",
    "that your decision stumps must support weighted training examples. Based on\n",
    "your decision stump learning algorithm, implement AdaBoost algorithm. Vary\n",
    "the number of iterations T from 1 to 1000, and examine the training and test\n",
    "errors. You should report the results in two figures. The first figure shows how the\n",
    "training and test errors vary along with T. The second figure shows the training\n",
    "and test errors of all the decision stumps learned in each iteration. What can you\n",
    "observe and conclude? You have had the results for a fully expanded decision tree\n",
    "in HW1. Comparing them with Adaboost, what can you observe and conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### B. Bagged Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8 points] Based on your code of the decision tree learning algorithm (with information gain), implement a Bagged trees learning algorithm. Note that each tree\n",
    "should be fully expanded — no early stopping or post pruning. Vary the number\n",
    "of trees from 1 to 1000, report how the training and test errors vary along with\n",
    "the tree number in a figure. Overall, are bagged trees better than a single tree?\n",
    "Are bagged trees better than Adaboost?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "### C. Optimal weight vector with analytical form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the bias and variance decomposition, we have justified why\n",
    "the bagging approach is more effective than a single classifier/predictor. Let us\n",
    "verify it in real data. Experiment with the following procedure.\n",
    "\n",
    "• REPEAT for 100 times\n",
    "\n",
    "• [STEP 1] Sample 1, 000 examples uniformly without replacement from the training datset\n",
    "\n",
    "• [STEP 2] Run your bagged trees learning algorithm based on the 1, 000 training examples and learn 1000 trees.\n",
    "\n",
    "• END REPEAT\n",
    "\n",
    "\n",
    "• Now you have 100 bagged predictors in hand. For comparison, pick the first\n",
    "tree in each run to get 100 fully expanded trees (i.e. single trees).\n",
    "\n",
    "\n",
    "• For each of the test example, compute the predictions of the 100 single trees.\n",
    "Take the average, subtract the ground-truth label, and take square to compute\n",
    "the bias term (see the lecture slides). Use all the predictions to compute the\n",
    "sample variance as the approximation to the variance term (if you forget\n",
    "what the sample variance is, check it out here). You now obtain the bias and\n",
    "variance terms of a single tree learner for one test example. You will need\n",
    "to compute them for all the test examples and then take average as your\n",
    "final estimation of the bias and variance terms for the single decision tree\n",
    "learner. You can add the two terms to obtain the estimation of the general\n",
    "squared error (that is, expected error w.r.t test examples). Now use your 100\n",
    "bagged predictors to do the same thing and estimate the general bias and\n",
    "variance terms, as well as the general squared error. Comparing the results\n",
    "of the single tree learner and the bagged trees, what can you conclude? What\n",
    "causes the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "### D. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [8 points] Implement the random forest algorithm as we discussed in our lecture.\n",
    "Vary the number of random trees from 1 to 1000. Note that you need to modify\n",
    "your tree learning algorithm to randomly select a subset of features before each\n",
    "split. Then use the information gain to select the best feature to split. Vary the\n",
    "size of the feature subset from {2, 4, 6}. Note that if your feature subset happen\n",
    "to be all the features used before, simply resample the subset. Report in a figure\n",
    "how the training and test errors vary along with the number of random trees for\n",
    "each feature subset size setting. How does the performance compare with bagged\n",
    "trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "### E. Single random tree vs whole forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6 points] Following (c), estimate the bias and variance terms, and the squared\n",
    "error for a single random tree and the whole forest. Comparing with the bagged\n",
    "trees, what do you observe? What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
